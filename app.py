from flask import Flask, render_template, request, jsonify, send_file
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import re
import threading
import json
import os
from datetime import datetime
import uuid

app = Flask(__name__)

# Glob√°ln√≠ store pro bƒõ≈æ√≠c√≠ √∫lohy
running_jobs = {}

class AIScraper:
    def __init__(self, job_id):
        self.job_id = job_id
        # Unik√°tn√≠ mno≈æina pro ukl√°d√°n√≠ hlavn√≠ch dom√©n
        self.unique_domains = set()
        # AI souvisej√≠c√≠ kl√≠ƒçov√° slova pro lep≈°√≠ detekci
        self.ai_keywords = ['ai', 'artificial', 'intelligence', 'tool', 'gpt', 'chat', 'bot', 'machine', 'learning', 'neural', 'openai', 'claude', 'gemini', 'copilot', 'assistant', 'generator', 'automation', 'smart', 'auto']
        # Seznam dom√©n k ignorov√°n√≠ (soci√°ln√≠ s√≠tƒõ, bƒõ≈æn√© weby)
        self.ignore_domains = ['facebook.com', 'twitter.com', 'linkedin.com', 'youtube.com', 'google.com', 'github.com']
        
        # Nastaven√≠ pro ukl√°d√°n√≠ v√Ωsledk≈Ø
        self.results_dir = "results"
        if not os.path.exists(self.results_dir):
            os.makedirs(self.results_dir)
        self.results_file = os.path.join(self.results_dir, f"{job_id}_results.json")
        self.status_file = os.path.join(self.results_dir, f"{job_id}_status.json")
        
        # Naƒçti existuj√≠c√≠ v√Ωsledky do unique_domains pro prevenci duplicit
        self.load_existing_results()
        
        # Ulo≈æen√≠ poƒç√°teƒçn√≠ho stavu
        self.update_status("starting", "Inicializace scraperu...")
        
    def load_existing_results(self):
        """Naƒçte existuj√≠c√≠ v√Ωsledky pro prevenci duplicit"""
        if os.path.exists(self.results_file):
            try:
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    existing_results = json.load(f)
                    for result in existing_results:
                        if result.get('url'):
                            self.unique_domains.add(result['url'])
                print(f"üîÑ Naƒçteno {len(self.unique_domains)} existuj√≠c√≠ch URL pro prevenci duplicit")
            except Exception as e:
                print(f"‚ö†Ô∏è  Nepoda≈ôilo se naƒç√≠st existuj√≠c√≠ v√Ωsledky: {e}")
                
    def update_status(self, status, message, found_count=0):
        """Aktualizuje stav √∫lohy"""
        status_data = {
            "status": status,  # starting, running, completed, error
            "message": message,
            "found_count": found_count,
            "timestamp": datetime.now().isoformat(),
            "job_id": self.job_id
        }
        
        with open(self.status_file, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, ensure_ascii=False, indent=2)
        
        # Tak√© aktualizuj glob√°ln√≠ store
        running_jobs[self.job_id] = status_data
        
    def save_batch(self, new_urls):
        """Ulo≈æ√≠ novou d√°vku URL do souboru s d≈Øslednou kontrolou duplicit"""
        if not new_urls:
            return
            
        # Naƒçti existuj√≠c√≠ v√Ωsledky
        all_results = []
        existing_urls = set()  # Mno≈æina pro rychlou kontrolu duplicit
        
        if os.path.exists(self.results_file):
            try:
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    all_results = json.load(f)
                    # Vytvo≈ô mno≈æinu existuj√≠c√≠ch URL pro rychlou kontrolu
                    existing_urls = {r.get('url') for r in all_results if r.get('url')}
            except:
                all_results = []
                existing_urls = set()
        
        # Poƒç√≠tadlo skuteƒçnƒõ nov√Ωch URL
        actually_new = 0
        
        # P≈ôidej pouze skuteƒçnƒõ nov√© URL
        for url in new_urls:
            if url not in existing_urls:
                all_results.append({
                    "url": url,
                    "found_at": datetime.now().isoformat()
                })
                existing_urls.add(url)  # P≈ôidej do mno≈æiny pro dal≈°√≠ kontroly
                actually_new += 1
                print(f"  ‚ûï Nov√©: {url}")
            else:
                print(f"  ‚ö†Ô∏è  Duplik√°t ignorov√°n: {url}")
        
        # Ulo≈æ zpƒõt jen pokud jsou skuteƒçnƒõ nov√° data
        if actually_new > 0:
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ Ulo≈æeno {actually_new} nov√Ωch URL. Celkem: {len(all_results)}")
        else:
            print(f"‚ÑπÔ∏è  ≈Ω√°dn√© nov√© URL k ulo≈æen√≠ - v≈°echny byly duplik√°ty")
        
    def is_ai_tool_domain(self, url):
        """Zjist√≠, zda je URL AI n√°stroj podle dom√©ny a kontextu"""
        try:
            domain = urlparse(url).netloc.lower()
            
            # Odstra≈à www. prefix
            if domain.startswith('www.'):
                domain = domain[4:]
            
            # Ignoruj zn√°m√© ne-AI dom√©ny a p≈Øvodn√≠ str√°nku
            ignore_domains = self.ignore_domains + ['futurepedia.io', 'theresanaiforthat.com', 'futuretools.io', 'producthunt.com']
            for ignore in ignore_domains:
                if ignore in domain:
                    return False
            
            # Roz≈°√≠≈ôen√Ω seznam prioritn√≠ch koncovek
            ai_extensions = ['.ai', '.io', '.app', '.tech', '.co']
            for ext in ai_extensions:
                if domain.endswith(ext):
                    return True
            
            # Pro .com, .org, .net dom√©ny - kontrola AI kl√≠ƒçov√Ωch slov
            common_extensions = ['.com', '.org', '.net', '.dev', '.cc', '.me', '.ly']
            for ext in common_extensions:
                if domain.endswith(ext):
                    for keyword in self.ai_keywords:
                        if keyword in domain:
                            return True
            
            # Pokud m√° URL suggestivn√≠ path (nap≈ô. obsahuje "tool", "ai")
            path = urlparse(url).path.lower()
            ai_path_keywords = ['tool', 'ai', 'gpt', 'chat', 'bot', 'generate', 'create']
            for keyword in ai_path_keywords:
                if keyword in path:
                    return True
            
            return False
            
        except Exception:
            return False
    
    def get_main_domain(self, url):
        """Vr√°t√≠ hlavn√≠ dom√©nu z URL"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            if domain.startswith('www.'):
                domain = domain[4:]
            return f"https://{domain}"
        except Exception:
            return None

    def extract_all_links_from_content(self, content, base_url):
        """Extrahuje v≈°echny mo≈æn√© odkazy z obsahu str√°nky s d≈Øslednou kontrolou duplicit"""
        found_urls = []
        local_found_domains = set()  # Lok√°ln√≠ kontrola duplicit pro tuto str√°nku
        
        # 1. Zkus naj√≠t URL v JSON datech
        json_patterns = [
            r'"websiteUrl":"(https?://[^"?]+)',
            r'"url":"(https?://[^"?]+)',
            r'"link":"(https?://[^"?]+)',
            r'"href":"(https?://[^"?]+)'
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, content)
            for url in matches:
                if self.is_ai_tool_domain(url):
                    main_domain = self.get_main_domain(url)
                    if main_domain and main_domain not in self.unique_domains and main_domain not in local_found_domains:
                        self.unique_domains.add(main_domain)
                        local_found_domains.add(main_domain)
                        found_urls.append(main_domain)
                        print(f"DEBUG: JSON AI n√°stroj: {main_domain}")
                    elif main_domain in self.unique_domains:
                        print(f"DEBUG: JSON duplik√°t ignorov√°n: {main_domain}")
        
        # 2. Hledej URL v textu (nap≈ô. api.domain.com v skriptech)
        url_pattern = r'https?://[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:/[^\s"\'<>]*)?'
        text_urls = re.findall(url_pattern, content)
        
        for url in text_urls:
            if self.is_ai_tool_domain(url):
                main_domain = self.get_main_domain(url)
                if main_domain and main_domain not in self.unique_domains and main_domain not in local_found_domains:
                    self.unique_domains.add(main_domain)
                    local_found_domains.add(main_domain)
                    found_urls.append(main_domain)
                    print(f"DEBUG: TEXT AI n√°stroj: {main_domain}")
                elif main_domain in self.unique_domains:
                    print(f"DEBUG: TEXT duplik√°t ignorov√°n: {main_domain}")
        
        print(f"DEBUG: Celkem nalezeno {len(found_urls)} NOV√ùCH AI n√°stroj≈Ø z obsahu (ignorov√°no {len(local_found_domains) - len(found_urls)} duplik√°t≈Ø)")
        return found_urls

    def scrape_page(self, url, max_depth=2, current_depth=0, test_mode=False):
        """Projde str√°nku a najde AI n√°stroje s asynchronn√≠m ukl√°d√°n√≠m"""
        print(f"Scrapuji: {url} (hloubka: {current_depth})")
        
        try:
            # Aktualizuj stav
            self.update_status("running", f"Scrapuji: {url} (hloubka: {current_depth})", len(self.unique_domains))
            
            # Z√≠sk√° obsah str√°nky
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            content = response.text
            found_urls = []
            
            # Nejd≈ô√≠ve zkus√≠ extrahovat z obsahu str√°nky (JSON + text)
            content_urls = self.extract_all_links_from_content(content, url)
            found_urls.extend(content_urls)
            
            # ULO≈ΩEN√ç D√ÅVKY PR≈ÆBƒö≈ΩNƒö (ka≈æd√Ωch 10+ URL)
            if len(content_urls) > 0:
                self.save_batch(content_urls)
                self.update_status("running", f"Nalezeno {len(content_urls)} AI n√°stroj≈Ø na {url}", len(self.unique_domains))
            
            # Pokud v test m√≥du najde dost URL z obsahu, zastav
            if test_mode and len(found_urls) >= 10:
                self.update_status("completed", f"TEST M√ìD: Nalezeno {len(found_urls)} AI n√°stroj≈Ø", len(self.unique_domains))
                return found_urls[:10]
            
            # Pokud nenajde dost v JSON nebo nen√≠ test m√≥d, pokraƒçuj bƒõ≈æn√Ωm scrapingem
            soup = BeautifulSoup(content, 'html5lib')
            links = soup.find_all('a', href=True)
            subpages_to_visit = []
            
            # V test m√≥du omez√≠ zpracov√°n√≠ na prvn√≠ch 50 odkaz≈Ø
            links_to_process = links[:50] if test_mode else links
            
            for link in links_to_process:
                href = link['href']
                
                # P≈ôevede relativn√≠ odkazy na absolutn√≠
                full_url = urljoin(url, href)
                
                # Kontrola, zda je to AI n√°stroj
                if self.is_ai_tool_domain(full_url):
                    main_domain = self.get_main_domain(full_url)
                    if main_domain and main_domain not in self.unique_domains:
                        self.unique_domains.add(main_domain)
                        found_urls.append(main_domain)
                        print(f"Nalezen AI n√°stroj: {main_domain}")
                        
                        # PR≈ÆBƒö≈ΩN√â UKL√ÅD√ÅN√ç po ka≈æd√Ωch 5 n√°lezech
                        if len(found_urls) % 5 == 0:
                            self.save_batch([main_domain])
                            
                        # V test m√≥du zastav po nalezen√≠ 10 n√°stroj≈Ø
                        if test_mode and len(found_urls) >= 10:
                            self.update_status("completed", "TEST M√ìD: Nalezeno 10 AI n√°stroj≈Ø", len(self.unique_domains))
                            self.save_batch(found_urls[-5:])  # ulo≈æ posledn√≠ch 5
                            return found_urls[:10]
                
                # Shrom√°≈æd√≠ podstr√°nky k prozkoum√°n√≠ (pouze ze stejn√© dom√©ny)
                elif current_depth < max_depth and not test_mode:  # V test m√≥du neproch√°zej podstr√°nky
                    if urlparse(full_url).netloc == urlparse(url).netloc:
                        if full_url not in subpages_to_visit and full_url != url:
                            subpages_to_visit.append(full_url)
            
            # Rekurzivnƒõ projde podstr√°nky (pokud nen√≠ test m√≥d)
            if current_depth < max_depth and not test_mode:
                for subpage in subpages_to_visit[:10]:  # Omez√≠ na 10 podstr√°nek pro rychlost
                    time.sleep(1)  # Pause mezi po≈æadavky
                    sub_results = self.scrape_page(subpage, max_depth, current_depth + 1, test_mode)
                    found_urls.extend(sub_results)
                    
                    # Ulo≈æ d√°vku po ka≈æd√© podstr√°nce
                    if sub_results:
                        self.save_batch(sub_results)
            
            return found_urls
            
        except Exception as e:
            error_msg = f"Chyba p≈ôi scrapov√°n√≠ {url}: {str(e)}"
            print(error_msg)
            self.update_status("error", error_msg, len(self.unique_domains))
            return []

    def run_scraping_job(self, start_url, test_mode=False):
        """Spust√≠ hlavn√≠ scraping √∫lohu"""
        try:
            self.update_status("running", f"Spou≈°t√≠m scraping {'v TEST m√≥du' if test_mode else 'v PLN√âM m√≥du'}...")
            
            results = self.scrape_page(start_url, max_depth=2, test_mode=test_mode)
            
            # Fin√°ln√≠ ulo≈æen√≠
            if results:
                self.save_batch(results)
            
            final_count = len(self.unique_domains)
            self.update_status("completed", f"Scraping dokonƒçen! Nalezeno {final_count} AI n√°stroj≈Ø.", final_count)
            
            return results
            
        except Exception as e:
            error_msg = f"Kritick√° chyba: {str(e)}"
            self.update_status("error", error_msg, len(self.unique_domains))
            return []

@app.route('/')
def index():
    """Hlavn√≠ str√°nka s formul√°≈ôem"""
    return render_template('index.html')

@app.route('/scrape', methods=['POST'])
def scrape():
    """Spust√≠ scraping v background threadu"""
    data = request.get_json()
    start_url = data.get('url')
    test_mode = data.get('test_mode', False)
    
    if not start_url:
        return jsonify({'error': 'URL nen√≠ zadan√©'}), 400
    
    try:
        # Vytvo≈ô unik√°tn√≠ ID pro √∫lohu
        job_id = str(uuid.uuid4())[:8]
        
        # Vytvo≈ô nov√Ω scraper
        scraper = AIScraper(job_id)
        
        # Spus≈• scraping v background threadu
        def run_background_scraping():
            scraper.run_scraping_job(start_url, test_mode)
        
        thread = threading.Thread(target=run_background_scraping)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'job_id': job_id,
            'message': f'Scraping spu≈°tƒõn v pozad√≠! ID √∫lohy: {job_id}',
            'status_url': f'/status/{job_id}',
            'results_url': f'/results/{job_id}'
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Chyba p≈ôi spu≈°tƒõn√≠: {str(e)}'
        }), 500

@app.route('/status/<job_id>')
def get_job_status(job_id):
    """Vr√°t√≠ aktu√°ln√≠ stav √∫lohy"""
    try:
        status_file = os.path.join("results", f"{job_id}_status.json")
        
        if not os.path.exists(status_file):
            return jsonify({
                'success': False,
                'message': '√öloha nenalezena'
            }), 404
        
        with open(status_file, 'r', encoding='utf-8') as f:
            status_data = json.load(f)
        
        return jsonify({
            'success': True,
            'status': status_data['status'],
            'message': status_data['message'],
            'found_count': status_data.get('found_count', 0),
            'timestamp': status_data['timestamp'],
            'job_id': job_id
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Chyba p≈ôi ƒçten√≠ stavu: {str(e)}'
        }), 500

@app.route('/results/<job_id>')
def get_job_results(job_id):
    """Vr√°t√≠ v√Ωsledky √∫lohy po d√°vk√°ch"""
    try:
        results_file = os.path.join("results", f"{job_id}_results.json")
        
        if not os.path.exists(results_file):
            return jsonify({
                'success': False,
                'message': 'V√Ωsledky nenalezeny'
            }), 404
        
        with open(results_file, 'r', encoding='utf-8') as f:
            all_results = json.load(f)
        
        # Parametry pro str√°nkov√°n√≠
        page = request.args.get('page', 1, type=int)
        per_page = request.args.get('per_page', 50, type=int)
        
        # V√Ωpoƒçet rozsahu
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        
        batch_results = all_results[start_idx:end_idx]
        has_more = end_idx < len(all_results)
        
        return jsonify({
            'success': True,
            'urls': [r['url'] for r in batch_results],
            'detailed_results': batch_results,
            'total_found': len(all_results),
            'page': page,
            'per_page': per_page,
            'has_more': has_more,
            'job_id': job_id
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Chyba p≈ôi ƒçten√≠ v√Ωsledk≈Ø: {str(e)}'
        }), 500

@app.route('/download/<job_id>')
def download_results(job_id):
    """St√°hne v√Ωsledky jako textov√Ω soubor"""
    try:
        results_file = os.path.join("results", f"{job_id}_results.json")
        
        if not os.path.exists(results_file):
            return jsonify({'error': 'V√Ωsledky nenalezeny'}), 404
        
        with open(results_file, 'r', encoding='utf-8') as f:
            all_results = json.load(f)
        
        # Vytvo≈ô textov√Ω soubor
        txt_content = "# AI N√°stroje nalezen√© scraperem\n\n"
        for i, result in enumerate(all_results, 1):
            txt_content += f"{i}. {result['url']}\n"
        
        txt_file = os.path.join("results", f"{job_id}_urls.txt")
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(txt_content)
        
        return send_file(txt_file, as_attachment=True, download_name=f"ai_tools_{job_id}.txt")
        
    except Exception as e:
        return jsonify({'error': f'Chyba p≈ôi stahov√°n√≠: {str(e)}'}), 500

@app.route('/jobs')
def list_jobs():
    """Zobraz√≠ seznam v≈°ech √∫loh"""
    try:
        if not os.path.exists("results"):
            return jsonify({'jobs': []})
        
        jobs = []
        for filename in os.listdir("results"):
            if filename.endswith("_status.json"):
                job_id = filename.replace("_status.json", "")
                
                with open(os.path.join("results", filename), 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
                
                jobs.append({
                    'job_id': job_id,
                    'status': status_data['status'],
                    'message': status_data['message'],
                    'found_count': status_data.get('found_count', 0),
                    'timestamp': status_data['timestamp']
                })
        
        # Se≈ôaƒè podle ƒçasu (nejnovƒõj≈°√≠ prvn√≠)
        jobs.sort(key=lambda x: x['timestamp'], reverse=True)
        
        return jsonify({'jobs': jobs})
        
    except Exception as e:
        return jsonify({'error': f'Chyba p≈ôi naƒç√≠t√°n√≠ √∫loh: {str(e)}'}), 500

if __name__ == '__main__':
    import os
    port = int(os.environ.get('PORT', 5001))
    print("Spou≈°t√≠m AI Scraper...")
    print(f"Otev≈ôete http://localhost:{port} ve va≈°em prohl√≠≈æeƒçi")
    app.run(host='0.0.0.0', port=port, debug=False) 